{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6863986,"sourceType":"datasetVersion","datasetId":3944908},{"sourceId":6972317,"sourceType":"datasetVersion","datasetId":4006080},{"sourceId":6972416,"sourceType":"datasetVersion","datasetId":4006158},{"sourceId":6973059,"sourceType":"datasetVersion","datasetId":4006604},{"sourceId":6981436,"sourceType":"datasetVersion","datasetId":4012123}],"dockerImageVersionId":30580,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#  https://www.kaggle.com/code/khanh123987/tabular-question-answering","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:12:08.565609Z","iopub.execute_input":"2023-11-16T13:12:08.565999Z","iopub.status.idle":"2023-11-16T13:12:08.570420Z","shell.execute_reply.started":"2023-11-16T13:12:08.565970Z","shell.execute_reply":"2023-11-16T13:12:08.569411Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():\n    print(\"GPU 已开启\")\n    device = torch.device(\"cuda\")\nelse:\n    print(\"GPU 未开启\")\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:12:08.574784Z","iopub.execute_input":"2023-11-16T13:12:08.575774Z","iopub.status.idle":"2023-11-16T13:12:08.611314Z","shell.execute_reply.started":"2023-11-16T13:12:08.575748Z","shell.execute_reply":"2023-11-16T13:12:08.610430Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"GPU 已开启\n","output_type":"stream"}]},{"cell_type":"code","source":"#指定设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:12:08.612883Z","iopub.execute_input":"2023-11-16T13:12:08.613193Z","iopub.status.idle":"2023-11-16T13:12:08.619410Z","shell.execute_reply.started":"2023-11-16T13:12:08.613168Z","shell.execute_reply":"2023-11-16T13:12:08.618595Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers --quiet\n!pip install datasets --quiet\n!pip install sentencepiece --quiet\n!pip install accelerate --quiet\n!pip install -U nltk\n!pip install sacrebleu --quiet\n!pip install bert_score --quiet\n!pip install git+https://github.com/google-research/bleurt.git --quiet","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:12:08.620520Z","iopub.execute_input":"2023-11-16T13:12:08.620767Z","iopub.status.idle":"2023-11-16T13:13:54.277969Z","shell.execute_reply.started":"2023-11-16T13:12:08.620746Z","shell.execute_reply":"2023-11-16T13:13:54.276700Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.3.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2023.8.8)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.1)\nInstalling collected packages: nltk\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nltk-3.8.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"transformers：用于自然语言处理和深度学习模型的Python库，包括预训练的语言模型（如BERT、GPT等）和用于文本分类、命名实体识别、问答等任务的工具。\n\ndatasets：提供了一个统一的API来加载和处理各种自然语言处理数据集。\n\nsentencepiece：用于训练和使用字节对编码（Byte Pair Encoding，BPE）模型的工具。\n\naccelerate：提供了加速深度学习模型训练的工具和框架，可利用多个GPU或TPU进行并行计算。\n\nnltk：自然语言工具包（Natural Language Toolkit），提供了各种文本处理和分析的功能。\n\nsacrebleu：用于计算机器翻译结果的BLEU评估指标的Python库。\n\nbert_score：计算BERT模型生成的句子相似性得分的Python库。\n\nbleurt：谷歌研究团队开发的用于评估文本生成质量的BERT模型。\n\n这些包的安装将为后续的代码执行提供必要的依赖项和工具。\n\n--quiet参数用于在安装过程中隐藏输出信息","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom torch.utils.data import DataLoader\nimport nltk\nnltk.download(\"punkt\", quiet=True)\n\nimport pandas as pd\nfrom datasets import load_dataset, load_metric\n\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    BartForConditionalGeneration,\n    DataCollatorForSeq2Seq,\n    HfArgumentParser,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    TapexTokenizer,\n    set_seed,\n)\nfrom transformers.trainer_utils import (\n    get_last_checkpoint, \n    is_main_process, \n    default_compute_objective,\n    # default_hp_space_ray\n)\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['WANDB_DISABLED'] = 'true'","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:13:54.281142Z","iopub.execute_input":"2023-11-16T13:13:54.281621Z","iopub.status.idle":"2023-11-16T13:14:06.308353Z","shell.execute_reply.started":"2023-11-16T13:13:54.281580Z","shell.execute_reply":"2023-11-16T13:14:06.307368Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"numpy：Python中处理数值计算的库。\n\ntorch：PyTorch深度学习框架的Python库。\n\nos：用于与操作系统进行交互的Python模块。\n\ntorch.nn：PyTorch提供的神经网络模块。\n\nmath：Python提供的数学运算模块。\n\nDataLoader：PyTorch提供的数据加载器。\n\nnltk：自然语言工具包（Natural Language Toolkit），提供了各种文本处理和分析的功能。\n\npandas：用于处理数据的Python库。\n\nload_dataset、load_metric：Hugging Face开源的数据集和评估指标库。\n\ntransformers：用于自然语言处理和深度学习模型的Python库，包括预训练的语言模型（如BERT、GPT等）和用于文本分类、命名实体识别、问答等任务的工具。\n\nAutoConfig、BartForConditionalGeneration、DataCollatorForSeq2Seq、HfArgumentParser、Seq2SeqTrainer、Seq2SeqTrainingArguments、TapexTokenizer：用于训练和生成seq2seq模型的相关类。\n\nset_seed、get_last_checkpoint、is_main_process、default_compute_objective：用于设置训练过程中的一些参数和函数。\n\nos.environ：用于设置环境变量的Python模块。\n\n总体而言，这段代码主要是为后续的seq2seq模型训练准备环境和依赖项。其中设置的环境变量可能与多线程/多进程相关，可以在后续的程序运行中提高效率。","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    set_seed(seed)\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n\nseed_everything(42)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:06.309525Z","iopub.execute_input":"2023-11-16T13:14:06.310115Z","iopub.status.idle":"2023-11-16T13:14:06.321577Z","shell.execute_reply.started":"2023-11-16T13:14:06.310085Z","shell.execute_reply":"2023-11-16T13:14:06.320384Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"定义了一个函数seed_everything()，用于设置随机数种子，以保证程序的可重复性。具体来说，该函数接受一个整数seed作为参数，并使用该种子值调用np.random.seed()、torch.manual_seed()、set_seed()等函数，以在多个随机数生成操作中使用相同的种子值。\n\n此外，函数还使用torch.cuda.is_available()判断当前是否有可用的GPU，如果有则继续调用torch.cuda.manual_seed()设置GPU的随机数种子，并将torch.backends.cudnn.deterministic设置为True，以保证在GPU上进行的计算结果可重复。\n\n最后，函数调用seed_everything(42)，将种子值设置为42，以在后续的程序执行中保证随机数的可重复性。","metadata":{}},{"cell_type":"code","source":"#data = pd.read_csv('/kaggle/input/policydata/policy_20231102170949.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:06.322959Z","iopub.execute_input":"2023-11-16T13:14:06.323303Z","iopub.status.idle":"2023-11-16T13:14:06.348553Z","shell.execute_reply.started":"2023-11-16T13:14:06.323272Z","shell.execute_reply":"2023-11-16T13:14:06.347681Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n\n# # 读取数据集\n# data = pd.read_csv('/kaggle/input/policydata/policy_20231102170949.csv')\n\n# # 统计每列的空值数量\n# null_counts = data.isnull().sum()\n\n# # 统计总共有多少行包含空值\n# rows_with_null = data.isnull().any(axis=1).sum()\n\n# # 输出每列的空值数量\n# print(\"每列的空值数量：\")\n# print(null_counts)\n\n# # 输出包含空值的行数\n# print(\"包含空值的行数：\", rows_with_null)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:06.349860Z","iopub.execute_input":"2023-11-16T13:14:06.350163Z","iopub.status.idle":"2023-11-16T13:14:06.359847Z","shell.execute_reply.started":"2023-11-16T13:14:06.350139Z","shell.execute_reply":"2023-11-16T13:14:06.359027Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"查看包含空值的列","metadata":{}},{"cell_type":"markdown","source":"使用了datasets库中的load_dataset()函数，用于加载数据集。具体而言，它从json文件中加载了三个数据集，分别是训练集、验证集和测试集。\n\nload_dataset()函数的第一个参数是数据集名称，这里使用了'json'。第二个参数data_files是一个字典，指定了各个数据集对应的文件路径，其中'train'对应训练集文件路径，'valid'对应验证集文件路径，'test'对应测试集文件路径。这里的文件路径分别为'/kaggle/input/tableqa/fetaQA-v1_train.json'、'/kaggle/input/tableqa/fetaQA-v1_dev.json'和'/kaggle/input/tableqa/fetaQA-v1_test.json'。\n\n最后一个参数field='data'指定了数据集中的字段名，这里使用了'data'作为字段名。\n\n通过上述代码，dataset变量将包含加载的数据集，可以通过dataset['train']、dataset['valid']和dataset['test']来访问对应的训练集、验证集和测试集数据。","metadata":{}},{"cell_type":"code","source":"#data","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:06.361083Z","iopub.execute_input":"2023-11-16T13:14:06.361642Z","iopub.status.idle":"2023-11-16T13:14:06.372489Z","shell.execute_reply.started":"2023-11-16T13:14:06.361609Z","shell.execute_reply":"2023-11-16T13:14:06.371603Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained('microsoft/tapex-base')\n\nconfig.no_repeat_ngram_size = 0\nconfig.max_length = 1024\nconfig.early_stopping = False\n\n# load tapex tokenizer\ntokenizer = TapexTokenizer.from_pretrained(#使用的是特殊的TapexTokenizer\n    'microsoft/tapex-base',\n    use_fast=True,\n    add_prefix_space=True\n)\n\n# load Bart based Tapex model (default tapex-large)\nmodel = BartForConditionalGeneration.from_pretrained('microsoft/tapex-base', config=config)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:06.373745Z","iopub.execute_input":"2023-11-16T13:14:06.374099Z","iopub.status.idle":"2023-11-16T13:14:13.588250Z","shell.execute_reply.started":"2023-11-16T13:14:06.374070Z","shell.execute_reply":"2023-11-16T13:14:13.587309Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/988 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db0da9e467e140f181fd2825d77cd6cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32b5bc1957d9403c849ca2f9b8255d9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65d8afc47ce74d4ebd111ba53e1c87c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9de161250dec4b508632a2d5fdddccb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42fddf530e594a10b107a4a66ac36577"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e2aab22dd224effad2ac219ffb2f1f8"}},"metadata":{}}]},{"cell_type":"markdown","source":"使用了Hugging Face的transformers库，通过调用相应的函数从预训练模型中加载配置信息、分词器和模型。\n\nTAPex模型基于BART架构，这是一种基于Transformer的序列到序列模型，同时具备自编码器和生成模型的能力。通过在大规模的代码库和自然语言文本数据上进行预训练，TAPex模型能够学习到代码和自然语言之间的关联信息。\n\n首先，使用AutoConfig.from_pretrained()函数从预训练模型'microsoft/tapex-base'加载配置信息，并将其存储在config变量中。no_repeat_ngram_size被设置为0，表示生成的文本中不会重复ngram。max_length被设置为1024，表示生成的文本的最大长度。early_stopping被设置为False，表示不启用提前停止策略。\n\n接下来，使用TapexTokenizer.from_pretrained()函数从预训练模型'microsoft/tapex-base'加载分词器，并将其存储在tokenizer变量中。use_fast=True表示使用快速分词器，add_prefix_space=True表示在分词时在句子前添加空格，以处理特定的语言模型需求。\n\n最后，使用BartForConditionalGeneration.from_pretrained()函数从预训练模型'microsoft/tapex-base'加载Bart based Tapex模型，并将其存储在model变量中。同时，将之前加载的配置信息config传递给模型。\n\n通过上述代码，我们可以使用tokenizer对文本进行分词，并使用model进行文本生成或其他相关任务。","metadata":{}},{"cell_type":"code","source":"# # 另一个模型\n# config = AutoConfig.from_pretrained(\"t5-base\")\n# tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n# model = AutoModelForSeq2SeqLM.from_pretrained(\n#     \"t5-base\",\n#     from_tf=False,\n#     config=config,\n# )","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:13.591682Z","iopub.execute_input":"2023-11-16T13:14:13.591971Z","iopub.status.idle":"2023-11-16T13:14:13.596350Z","shell.execute_reply.started":"2023-11-16T13:14:13.591946Z","shell.execute_reply":"2023-11-16T13:14:13.595303Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# 从原始文件读取表格数据\ndf = pd.read_csv(\"/kaggle/input/policydataset/policy_20231102170949.csv\")\n\n# 提取前100行数据\ndf_head = df.head(100)\n\n# 将提取的数据保存到另一个文件\ndf_head.to_csv(\"/kaggle/working/policy_100.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:42.972910Z","iopub.execute_input":"2023-11-16T13:14:42.973669Z","iopub.status.idle":"2023-11-16T13:14:56.076311Z","shell.execute_reply.started":"2023-11-16T13:14:42.973638Z","shell.execute_reply":"2023-11-16T13:14:56.075512Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"这个是为了将数据集缩小，用于先跑通代码","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# 读取CSV文件并转换为DataFrame\ndata = pd.read_csv('/kaggle/working/policy_100.csv')#要进行真正训练时需要修改\n\ndef preprocess_examples(examples):\n    questions = examples['title']\n    tables = pd.DataFrame({\n#         'id': examples['id'],\n        'city': examples['city'],\n        'title': examples['title'],\n        #'sub_title': examples['sub_title'],\n        #'pub_no': examples['pub_no'],\n        'pub_date': examples['pub_date'],\n        #'effect_date': examples['effect_date']\n    })\n    answers = examples['content']  # 将\"content\"列作为\"answer\"\n\n    model_inputs = tokenizer(\n        table=tables, query=questions, max_length=1024, padding=\"max_length\", truncation=True\n    )\n    labels = tokenizer(\n        answer=answers,\n        max_length=64,\n        padding=\"max_length\",\n        truncation=True,\n    )\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n    ]\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n\n    return model_inputs\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:15:25.205964Z","iopub.execute_input":"2023-11-16T13:15:25.206350Z","iopub.status.idle":"2023-11-16T13:15:25.294950Z","shell.execute_reply.started":"2023-11-16T13:15:25.206320Z","shell.execute_reply":"2023-11-16T13:15:25.293932Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"上面的data里的没有question列，尝试将问题列改为title列，并且table中有些数据是空值，现在是将这些有空值的列没有进行考虑","metadata":{}},{"cell_type":"markdown","source":"实现了一个名为preprocess_examples的函数，用于将给定的输入数据进行预处理以供模型训练。以下是代码实现的主要步骤：\n\n将问题文本转换为小写字母，并将其保存在名为questions的列表中。（但是文本中并没有问题文本，用title列进行替代）\n\n提取每个例子中的表格数据，并将其转换为pandas DataFrame格式。将所有数据表保存在名为tables的列表中。（table表格中也对一些具有空值的数进行了不考虑处理）\n\n从输入数据中提取答案，并将其保存在名为answers的列表中。\n\n通过调用一个名为tokenizer的分词器，将表格数据和问题文本编码为模型的输入。同时，使用padding和truncation参数来确保所有模型输入都是相同的长度，并将结果保存在名为model_inputs的字典中。\n\n使用tokenizer再次对答案进行编码，并将结果保存在名为labels的字典中。同时，通过将标签中的pad_token_id替换为-100，创建一个mask以指示哪些元素是真实的标签（值为1）和哪些元素是填充的（值为0）。\n\n最后，将labels中的input_ids赋值给model_inputs中的labels，以便在模型训练时使用。\n\n总之，该函数将输入数据预处理为模型的输入和输出，并返回一个包含这些数据的字典。","metadata":{}},{"cell_type":"code","source":"# print(data.columns)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.609323Z","iopub.status.idle":"2023-11-16T13:14:14.609696Z","shell.execute_reply.started":"2023-11-16T13:14:14.609502Z","shell.execute_reply":"2023-11-16T13:14:14.609519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n# 对CSV中的数据进行预处理\n#encoded_train_ds = preprocess_examples(data)\n# 如果需要转换成Hugging Face Datasets格式，可以使用以下代码\nencoded_train_ds = Dataset.from_pandas(data)  # 将DataFrame转换为Hugging Face Datasets格式\nencoded_train_ds = encoded_train_ds.map(preprocess_examples, batched=True, remove_columns=encoded_train_ds.column_names)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:15:34.380563Z","iopub.execute_input":"2023-11-16T13:15:34.381218Z","iopub.status.idle":"2023-11-16T13:15:49.158740Z","shell.execute_reply.started":"2023-11-16T13:15:34.381182Z","shell.execute_reply":"2023-11-16T13:15:49.157852Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n  if _pandas_api.is_sparse(col):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de3d2d5d4b474b3eb6e4d1feb6848b58"}},"metadata":{}}]},{"cell_type":"markdown","source":"使用了datasets库中的map()函数，用于将原始数据集中的每个样本映射到经过处理的样本上，并返回一个新的数据集。具体而言，它使用preprocess_examples()函数对训练集、验证集和测试集进行了预处理，将原始数据集转换为模型需要的输入格式，并将处理后的结果存储在encoded_train_ds、encoded_val_ds和encoded_test_ds变量中。（目前是将数据集转换为训练集，结果存储在encoded_train_ds里）\n\nmap()函数的第一个参数是一个函数，用于对原始数据集中的每个样本进行处理。这里使用了preprocess_examples作为处理函数。\n\nbatched=True表示对数据集进行批处理，默认情况下，map()函数会处理单个样本，但启用批处理可以提高处理效率。\n\nremove_columns参数指定要从数据集中删除的列，这里我们删除了原始数据集中的所有列，只保留了处理后的列。\n\n通过上述代码，我们可以将原始数据集转换为模型需要的输入格式，并将其存储在新的数据集中，以便后续训练和评估。\n","metadata":{}},{"cell_type":"code","source":"tokenizer.decode(encoded_train_ds[0]['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:15:55.847662Z","iopub.execute_input":"2023-11-16T13:15:55.848352Z","iopub.status.idle":"2023-11-16T13:15:55.870515Z","shell.execute_reply.started":"2023-11-16T13:15:55.848314Z","shell.execute_reply":"2023-11-16T13:15:55.869494Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'<s> 宁夏回族自治区人民政府立法工作规定 col : city | title | pub_date row 1 : nx |  宁夏回族自治 | 2022-12-23 row 2 : nx |  宁夏回族自治 | 2022-9-30 row 3 : nx |  宁夏回族自治 | 2022-1-8 row 4 : nx |  宁夏回族自治 | 2021-7-29 row 5 : nx |  宁夏回族自治 | 2020-12-22 row 6 : nx |  宁夏回族自治 | 2020-12-22 row 7 : nx |  宁夏回族自治 | 2020-3-18 row 8 : nx |  宁夏回族自治 | 2020-1-9 row 9 : nx |  宁夏回族自治 | 2020-1-9 row 10 : nx |  宁夏回族自治 | 2019-12-31 row 11 : nx |  宁夏回族自治 | 2019-5-8 row 12 : nx |  宁夏回族自治 | 2019-3-2 row 13 : nx |  宁夏回族自治 | 2018-12-24 row 14 : nx |  宁夏回族自治 | 2018-12-24 row 15 : nx |  宁夏回族自治 | 2018-12-24 row 16 : nx |  宁夏回族自治 | 2018-9-4 row 17 : nx |  宁夏回族自治 | 2018-1-4 row 18 : nx |  宁夏回族自治 | 2018-1-4 row 19 : nx |  宁夏回族自治 | 2017-12-21 row 20 : nx |  宁夏回族自治 | 2017-12-21 row 21 : nx |  宁夏回族自治 | 2017-11-30 row 22 : nx |  宁夏回族自治 | 2017-11-17 row 23 : nx |  宁夏回族自治 | 2017-1-20 row 24 : nx |  宁夏回族自治 | 2017-1-12 row 25 : nx |  宁夏回族自治 | 2016-12-23 row 26 : nx |  宁夏回族自治 | 2016-9-2 row 27 : nx |  宁夏回族自治 | 2016-8-17 row 28 : nx |  宁夏回族自治 | 2016-7-12 row 29 : nx |  宁夏回族自治 | 2015-12-13 row 30 : nx |  宁夏回族自治 | 2015-12-8 row 31 : nx |  宁夏回族自治 | 2015-12-8 row 32 : nx |  宁夏回族自治 | 2015-11-14 row 33 : nx |  宁夏回族自治 | 2015-10-12 row 34 : nx |  宁夏回族自治 | 2015-1-10 row 35 : nx |  宁夏回族自治 | 2015</s>'"},"metadata":{}}]},{"cell_type":"markdown","source":"使用了之前加载的分词器tokenizer，通过调用decode()函数将经过编码后的样本数据转换回文本格式。\n\n具体而言，它对encoded_train_ds[0]['input_ids']这个已编码的样本数据进行解码，将其转换为原始文本格式。encoded_train_ds[0]表示训练集中的第一个样本，['input_ids']表示该样本的编码输入部分。\n\n通过查看这个解码后的文本，我们可以更好地理解模型处理数据的方式，并检查预处理和编码是否正确。","metadata":{}},{"cell_type":"code","source":"tokenizer.decode([x for x in encoded_train_ds[0]['labels'] if x != -100])","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:15:59.419924Z","iopub.execute_input":"2023-11-16T13:15:59.420299Z","iopub.status.idle":"2023-11-16T13:15:59.431086Z","shell.execute_reply.started":"2023-11-16T13:15:59.420271Z","shell.execute_reply":"2023-11-16T13:15:59.430102Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'<s> 宁夏回族自治区人民政府立法工作规定\\n2022年12月23日宁夏回族自治</s>'"},"metadata":{}}]},{"cell_type":"markdown","source":"encoded_train_ds[0]['labels']是一个包含标签的列表。在这里，我们使用了列表推导式来过滤掉值为-100的标签，并将剩余的标签传递给tokenizer.decode()函数进行解码。\n\ntokenizer.decode()函数将经过过滤的标签列表作为输入，将其转换为原始文本格式。\n\n通过执行这段代码，我们可以查看第一个训练样本的标签在解码后的文本表示中的内容。这对于理解标签和模型输出之间的对应关系很有帮助。","metadata":{}},{"cell_type":"code","source":"import json\ndef save_json(content, path, indent=4, **json_dump_kwargs):\n    with open(path, \"w\") as f:\n        json.dump(content, f, indent=indent, sort_keys=True, **json_dump_kwargs)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:16:02.707400Z","iopub.execute_input":"2023-11-16T13:16:02.707772Z","iopub.status.idle":"2023-11-16T13:16:02.713120Z","shell.execute_reply.started":"2023-11-16T13:16:02.707740Z","shell.execute_reply":"2023-11-16T13:16:02.712039Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"定义了一个名为save_json()的函数，用于将Python对象保存为JSON格式的文件。\n\n函数接受三个参数：\n\ncontent：要保存为JSON的Python对象。\npath：要保存JSON文件的路径。\nindent：可选参数，表示JSON文件的缩进空格数，默认为4。\n**json_dump_kwargs：可选参数，用于传递给json.dump()函数的其他关键字参数。\n在函数内部，使用open()函数以写入模式打开指定路径的文件，并将文件对象赋值给变量f。然后，使用json.dump()函数将Python对象content转换为JSON格式，并将结果写入文件中。indent参数指定缩进空格数，sort_keys=True表示按照键的字典序对JSON进行排序。**json_dump_kwargs用于传递其他关键字参数，如果有的话。\n\n通过调用save_json()函数，你可以将Python对象保存为JSON文件，可以根据需要调整缩进、排序和其他相关的JSON保存选项。\n\n这个只定义了一下函数，具体在下文才用上","metadata":{}},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=-100,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:16:06.405136Z","iopub.execute_input":"2023-11-16T13:16:06.405834Z","iopub.status.idle":"2023-11-16T13:16:06.409922Z","shell.execute_reply.started":"2023-11-16T13:16:06.405785Z","shell.execute_reply":"2023-11-16T13:16:06.408948Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"定义了一个名为DataCollatorForSeq2Seq的数据收集器，用于将多个序列样本组合成批次，并进行填充和截断以满足模型的需求。\n\nDataCollatorForSeq2Seq接受三个参数：\n\ntokenizer：用于将原始文本转换为模型需要的输入格式。\nmodel：用于确定最大输入长度的模型实例。\nlabel_pad_token_id：标签数据集中要用作填充令牌的ID。\n在这里，我们使用DataCollatorForSeq2Seq来处理序列到序列模型的训练数据。通过指定上述三个参数，数据收集器将利用分词器tokenizer将原始文本转换为模型需要的输入格式，并使用模型实例model确定最大输入长度。同时，由于这是一个序列到序列模型，我们还需要指定标签数据集中用作填充令牌的ID。\n\n通过调用DataCollatorForSeq2Seq函数，我们可以有效地处理多个序列样本，并以批量方式提供给模型进行训练。","metadata":{}},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:16:10.268247Z","iopub.execute_input":"2023-11-16T13:16:10.269123Z","iopub.status.idle":"2023-11-16T13:16:24.185095Z","shell.execute_reply.started":"2023-11-16T13:16:10.269087Z","shell.execute_reply":"2023-11-16T13:16:24.184050Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.8.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.24.3)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (1.3.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (2023.8.8)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (4.66.1)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=250c8d2df62ef189c5736494b0ffecd8181d718a244833d5fdffb84ba6d47e4c\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_metric\nimport nltk\n\n# 加载 ROUGE metric\nrouge_metric = load_metric(\"rouge\")\n\n# 根据模型输出和真实标签进行文本后处理\ndef postprocess_text(preds, labels, metric_name):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    # 对于 ROUGE 指标，需要在每个句子后面增加换行符\n    if metric_name == \"rouge\":\n        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n    elif metric_name == \"sacrebleu\":  # sacrebleu 指标\n        labels = [[label] for label in labels]\n    elif metric_name == \"bleu\":\n        preds = [pred.split(' ') for pred in preds]\n        labels = [[label.split(' ')] for label in labels]\n    else:\n        pass\n\n    return preds, labels","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:16:27.530591Z","iopub.execute_input":"2023-11-16T13:16:27.530968Z","iopub.status.idle":"2023-11-16T13:16:28.158392Z","shell.execute_reply.started":"2023-11-16T13:16:27.530939Z","shell.execute_reply":"2023-11-16T13:16:28.157501Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63766fb539a24ed58f26a2d1f3dd89f7"}},"metadata":{}}]},{"cell_type":"markdown","source":"定义了一个名为postprocess_text()的后处理函数，用于对模型生成的文本进行一些额外的处理，以便与参考标签进行比较。\n\n函数接受三个参数：\n\npreds：模型生成的文本列表。\nlabels：参考标签的文本列表。\nmetric_name：评估指标的名称，可以是\"rouge\"、\"sacrebleu\"或\"bleu\"。\n在函数内部，首先使用列表推导式对preds和labels中的文本进行去除首尾空格的操作，将其存储在preds和labels变量中。\n\n然后，根据metric_name的值，进行不同的后处理操作：\n\n如果metric_name是\"rouge\"，则使用NLTK库的sent_tokenize()函数将每个预测和参考标签的文本分割成句子，并使用换行符连接这些句子，存储在preds和labels变量中。\n如果metric_name是\"sacrebleu\"，将参考标签的文本列表中的每个标签放入一个单独的列表中，存储在labels变量中。\n如果metric_name是\"bleu\"，将每个预测文本按空格分割成单词列表，将参考标签的文本列表中的每个标签按空格分割成单词列表的列表，分别存储在preds和labels变量中。\n如果metric_name不是以上三种情况，则不进行任何处理。\n最后，返回经过后处理的preds和labels列表。\n\n通过调用postprocess_text()函数，可以对模型生成的文本进行适当的后处理，以便与参考标签进行评估。根据所使用的评估指标，可以选择执行不同的后处理操作。","metadata":{}},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"/kaggle/working/\",\n    overwrite_output_dir=False,\n    do_train=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    warmup_steps=10,\n    weight_decay=0.01,\n    predict_with_generate=True,\n    learning_rate=0.1,\n    optim='adamw_torch',\n    save_strategy='epoch',\n    evaluation_strategy='no',#之后改为no\n    seed=42,\n    report_to='none',\n    #max_steps=500,  # 将值设置为 500\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:16:34.535135Z","iopub.execute_input":"2023-11-16T13:16:34.535982Z","iopub.status.idle":"2023-11-16T13:16:34.547471Z","shell.execute_reply.started":"2023-11-16T13:16:34.535950Z","shell.execute_reply":"2023-11-16T13:16:34.546651Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"（训练时参数需要进行修改）\n\n定义了一个名为Seq2SeqTrainingArguments的训练参数对象，用于配置Seq2Seq模型的训练过程。\n\n对象接受多个参数：\n\noutput_dir：训练输出结果的目录路径。\noverwrite_output_dir：如果output_dir已存在，是否覆盖。\ndo_train、do_eval、do_predict：是否分别执行训练、评估和预测。\nnum_train_epochs：训练轮数。\nper_device_train_batch_size：每个GPU设备的训练批次大小。\nper_device_eval_batch_size：每个GPU设备的评估批次大小。\nwarmup_steps：优化器增加学习率的热身步骤数。\nweight_decay：优化器中权重衰减的比例。\npredict_with_generate：是否使用生成模式进行预测。\nlearning_rate：优化器的学习率。\noptim：优化器的类型。\nsave_strategy：保存检查点的策略。可以是steps（每隔一定步数保存一次）或epoch（每个 epoch 结束时保存）。\nevaluation_strategy：评估策略。可以是steps（每隔一定步数评估一次）或epoch（每个 epoch 结束时评估）。\nseed：随机数种子。\n通过创建Seq2SeqTrainingArguments对象，可以灵活地配置Seq2Seq模型的训练过程。在实际的训练过程中，该对象将被传递给Seq2SeqTrainer类的构造函数，用于控制训练、评估和预测的行为。","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n      preds = preds[0]\n    decoded_raw_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_raw_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    dic_pred_label = {'predictions': decoded_raw_preds, 'labels': decoded_raw_labels}\n    save_json(dic_pred_label, os.path.join(training_args.output_dir, \"detokenized_outputs.json\"))\n\n    result = {}\n    # Only compute 'bertscore' metric\n    metric_name = \"bertscore\"\n    metric = load_metric(metric_name)\n    decoded_preds, decoded_labels = postprocess_text(decoded_raw_preds, decoded_raw_labels, metric_name)\n    res = metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n    for k,v in res.items():\n      if k ==\"hashcode\":\n        continue\n      result[f\"{metric_name}_{k}_0\"] = round(v[0], 2)\n      result[f\"{metric_name}_{k}_1\"] = round(v[1], 2)\n\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:16:38.051264Z","iopub.execute_input":"2023-11-16T13:16:38.051634Z","iopub.status.idle":"2023-11-16T13:16:38.061242Z","shell.execute_reply.started":"2023-11-16T13:16:38.051605Z","shell.execute_reply":"2023-11-16T13:16:38.060337Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"定义了一个名为compute_metrics()的函数，用于计算Seq2Seq模型评估指标。\n\n函数接受一个参数eval_preds，表示模型的预测结果和对应的标签。其中preds表示预测结果，labels表示对应的标签。函数首先使用batch_decode()方法将preds和labels转换为原始文本，并将其保存到一个字典中。该字典中包含两个键 predictions 和 labels，分别对应于预测的文本和真实标签的文本。\n\n接下来，函数对原始文本进行一些简单的后处理，以获取多个评估指标，如meteor、bleu、sacrebleu、bertscore和bleurt。对于每个指标，函数使用相应的load_metric()方法加载指标，并调用compute()方法计算预测结果和标签之间的指标值。对于bertscore指标，函数还需要指定语言类型（lang=\"en\"）。\n\n最后，函数计算生成文本的平均长度，并将所有指标值舍入到4位小数。返回值是一个字典，包含各个指标的名称和值。","metadata":{}},{"cell_type":"code","source":" from sklearn.model_selection import train_test_split\n\n# # 划分训练数据集和验证数据集\n#encoded_train_ds, eval_train_ds = train_test_split(encoded_train_ds, test_size=0.1, random_state=42)\n# #划分一小部分验证集，用来改正错误，但是验证集依然没有被使用\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_train_ds,\n    eval_dataset=None,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:16:44.180394Z","iopub.execute_input":"2023-11-16T13:16:44.181298Z","iopub.status.idle":"2023-11-16T13:16:49.254829Z","shell.execute_reply.started":"2023-11-16T13:16:44.181263Z","shell.execute_reply":"2023-11-16T13:16:49.253836Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"创建了一个名为trainer的Seq2SeqTrainer对象，用于训练和评估Seq2Seq模型。\n\nSeq2SeqTrainer类接受多个参数：\n\nmodel：要训练的Seq2Seq模型。\nargs：包含训练参数的对象，通常是Seq2SeqTrainingArguments类型的对象。\ntrain_dataset：用于训练的数据集，通常是经过编码处理后的训练数据集。\neval_dataset：用于评估的数据集，通常是经过编码处理后的验证数据集。\ntokenizer：用于将原始文本转换为模型输入的分词器。\ndata_collator：用于将样本批次组合成适合模型输入的数据集处理器。\n通过创建Seq2SeqTrainer对象，可以方便地配置并执行Seq2Seq模型的训练和评估过程。调用trainer.train()方法可以开始训练过程，调用trainer.evaluate()方法可以对模型进行评估，并根据需要保存模型的检查点和结果输出。","metadata":{}},{"cell_type":"code","source":"# last_checkpoint = get_last_checkpoint(training_args.output_dir)\n# print(last_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.630272Z","iopub.status.idle":"2023-11-16T13:14:14.630719Z","shell.execute_reply.started":"2023-11-16T13:14:14.630475Z","shell.execute_reply":"2023-11-16T13:14:14.630498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"首先调用自定义函数get_last_checkpoint()，并传入训练参数对象training_args.output_dir作为参数。training_args.output_dir表示输出目录的路径，通常是存储训练过程中生成的检查点文件和其他输出文件的位置。\n\nget_last_checkpoint()函数的作用是在指定的输出目录中查找以\"checkpoint-\"开头的文件，并返回最新的检查点文件的完整路径。如果找不到符合条件的检查点文件，则返回None。\n\n接下来，将获取到的最新检查点文件的路径存储在变量last_checkpoint中。\n\n最后，通过使用print()函数将last_checkpoint打印出来，以显示最新的检查点文件路径。这样可以方便查看最新的检查点文件在哪个位置。","metadata":{}},{"cell_type":"code","source":"# dataset_length = len(encoded_train_ds)\n# print(\"Dataset length:\", dataset_length)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.634126Z","iopub.status.idle":"2023-11-16T13:14:14.634492Z","shell.execute_reply.started":"2023-11-16T13:14:14.634315Z","shell.execute_reply":"2023-11-16T13:14:14.634333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# index = 1\n# if index >= dataset_length:\n#     print(\"Index is out of range.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.635798Z","iopub.status.idle":"2023-11-16T13:14:14.636196Z","shell.execute_reply.started":"2023-11-16T13:14:14.636005Z","shell.execute_reply":"2023-11-16T13:14:14.636022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"last_checkpoint = get_last_checkpoint(training_args.output_dir)\nprint(last_checkpoint)\n\nall_metrics = {}\n\nif training_args.do_train:\n    if last_checkpoint is not None:\n        checkpoint = last_checkpoint\n    elif os.path.isdir(\"tapex-base\"):\n        checkpoint = \"tapex-base\"\n    else:\n        checkpoint = None\n\n    if checkpoint is None:\n        # 处理键不存在的情况，执行模型训练\n        train_result = trainer.train()\n        trainer.save_model()  # 保存模型和分词器\n\n        metrics = train_result.metrics\n        max_train_samples = len(encoded_train_ds)\n        metrics[\"train_samples\"] = min(max_train_samples, len(encoded_train_ds))\n\n        if trainer.is_world_process_zero():\n            metrics_formatted = trainer.metrics_format(metrics)\n            print(\"***** train metrics *****\")\n            k_width = max(len(str(x)) for x in metrics_formatted.keys())\n            v_width = max(len(str(x)) for x in metrics_formatted.values())\n            for key in sorted(metrics_formatted.keys()):\n                print(f\"  {key: <{k_width}} = {metrics_formatted[key]:>{v_width}}\")\n            save_json(metrics, os.path.join(training_args.output_dir, \"train_results.json\"))\n            all_metrics.update(metrics)\n\n            # 需要保存训练状态，因为Trainer.save_model只会保存模型和分词器\n            trainer.state.save_to_json(os.path.join(training_args.output_dir, \"trainer_state.json\"))\n    else:\n        # 处理键存在的情况，执行恢复训练\n        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n        trainer.save_model()  # 保存模型和分词器\n\n        metrics = train_result.metrics\n        max_train_samples = len(encoded_train_ds)\n        metrics[\"train_samples\"] = min(max_train_samples, len(encoded_train_ds))\n\n        if trainer.is_world_process_zero():\n            metrics_formatted = trainer.metrics_format(metrics)\n            print(\"***** train metrics *****\")\n            k_width = max(len(str(x)) for x in metrics_formatted.keys())\n            v_width = max(len(str(x)) for x in metrics_formatted.values())\n            for key in sorted(metrics_formatted.keys()):\n                print(f\"  {key: <{k_width}} = {metrics_formatted[key]:>{v_width}}\")\n            save_json(metrics, os.path.join(training_args.output_dir, \"train_results.json\"))\n            all_metrics.update(metrics)\nelse:\n    checkpoint = None  # 处理 training_args.do_train 为 False 的情况，将 checkpoint 设置为 None","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:16:56.086476Z","iopub.execute_input":"2023-11-16T13:16:56.086970Z","iopub.status.idle":"2023-11-16T13:17:35.091044Z","shell.execute_reply.started":"2023-11-16T13:16:56.086904Z","shell.execute_reply":"2023-11-16T13:17:35.090072Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:35, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"***** train metrics *****\n  epoch                    =        3.0\n  total_flos               =   170358GF\n  train_loss               =    39.7716\n  train_runtime            = 0:00:37.38\n  train_samples            =        100\n  train_samples_per_second =      8.025\n  train_steps_per_second   =      2.006\n","output_type":"stream"}]},{"cell_type":"markdown","source":"主要是针对训练阶段的处理。首先，创建一个空字典all_metrics，用于存储所有的指标。\n\n接下来，根据训练参数training_args中的设定，判断是否需要进行训练。如果training_args.do_train为True，则进入训练流程。在训练流程中，首先判断最新的检查点文件路径last_checkpoint是否存在。如果存在，则将该路径赋值给变量checkpoint；如果不存在，则判断是否存在名为\"tapex-base\"的目录，如果存在，则将\"tapex-base\"赋值给checkpoint，否则将其设置为None。\n\n随后，调用trainer.train()方法开始训练过程。通过传递resume_from_checkpoint=checkpoint参数，可以在上次训练的检查点处继续训练。训练完成后，使用trainer.save_model()保存模型及其相关的分词器，以便后续上传等操作。\n\n接下来，从训练结果train_result中获取指标信息，并将最大训练样本数max_train_samples设置为编码后训练数据集的长度。然后，将指标train_samples设置为实际训练样本数和max_train_samples中较小的值。\n\n如果当前进程为主进程（trainer.is_world_process_zero()为True），则将指标结果进行格式化，并打印出来。然后，将指标保存为JSON文件并更新到all_metrics字典中。\n\n最后，为了保存完整的训练状态，还需要将trainer.state保存为JSON文件，文件路径为os.path.join(training_args.output_dir, \"trainer_state.json\")。这样做可以在需要恢复训练时，使用TrainerState.load_from_json()方法加载训练状态。","metadata":{}},{"cell_type":"code","source":"# last_checkpoint = get_last_checkpoint(training_args.output_dir)\n# checkpoint = last_checkpoint","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.640676Z","iopub.status.idle":"2023-11-16T13:14:14.641175Z","shell.execute_reply.started":"2023-11-16T13:14:14.640928Z","shell.execute_reply":"2023-11-16T13:14:14.640951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"先调用自定义函数get_last_checkpoint()，并传入训练参数对象training_args.output_dir作为参数，从输出目录中获取最新的检查点文件路径，将其赋值给变量last_checkpoint。\n\n接下来，将last_checkpoint赋值给变量checkpoint，以便在训练过程中使用该检查点文件路径。","metadata":{}},{"cell_type":"code","source":"# trainer = Seq2SeqTrainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=encoded_train_ds,\n#     tokenizer=tokenizer,\n#     data_collator=data_collator,\n#     compute_metrics=compute_metrics\n# )","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.642678Z","iopub.status.idle":"2023-11-16T13:14:14.643158Z","shell.execute_reply.started":"2023-11-16T13:14:14.642926Z","shell.execute_reply":"2023-11-16T13:14:14.642948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"创建了一个Seq2SeqTrainer对象trainer，用于训练序列到序列（seq2seq）模型。\n\n构造函数的参数包括：\n\nmodel：需要训练的模型。\nargs：训练参数对象。\ntrain_dataset：经过编码后的训练数据集。\neval_dataset：经过编码后的验证数据集。\ntokenizer：用于对文本进行分词和编码的分词器。\ndata_collator：用于处理训练数据的数据收集器。\ncompute_metrics：用于计算评估指标的函数。\n通过这些参数，Seq2SeqTrainer对象可以管理整个训练过程，并提供了训练、评估、保存模型等功能。","metadata":{}},{"cell_type":"code","source":"# trainer.train(resume_from_checkpoint=checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.644170Z","iopub.status.idle":"2023-11-16T13:14:14.644508Z","shell.execute_reply.started":"2023-11-16T13:14:14.644328Z","shell.execute_reply":"2023-11-16T13:14:14.644344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"调用trainer.train()方法开始训练过程。通过传递resume_from_checkpoint=checkpoint参数，可以在上次训练的检查点处继续训练。\n\n在训练过程中，trainer对象将根据训练参数中指定的训练步数、学习率等进行参数更新，并计算损失、评估指标等信息。训练过程中会自动进行反向传播、梯度更新等操作，直到达到指定的训练步数或满足其他停止条件。\n\n请注意，这里的checkpoint是之前获取到的最新检查点文件路径，用于恢复上次训练的状态。如果没有找到有效的检查点文件路径，则会从头开始训练。","metadata":{}},{"cell_type":"code","source":"# 定义保存路径\noutput_model_dir = \"/path/to/save/model\"\n\n# 保存模型和tokenizer\nmodel.save_pretrained(output_model_dir)\ntokenizer.save_pretrained(output_model_dir)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:17:42.573886Z","iopub.execute_input":"2023-11-16T13:17:42.574265Z","iopub.status.idle":"2023-11-16T13:17:43.824908Z","shell.execute_reply.started":"2023-11-16T13:17:42.574237Z","shell.execute_reply":"2023-11-16T13:17:43.823752Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"('/path/to/save/model/tokenizer_config.json',\n '/path/to/save/model/special_tokens_map.json',\n '/path/to/save/model/vocab.json',\n '/path/to/save/model/merges.txt',\n '/path/to/save/model/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.648284Z","iopub.status.idle":"2023-11-16T13:14:14.648635Z","shell.execute_reply.started":"2023-11-16T13:14:14.648465Z","shell.execute_reply":"2023-11-16T13:14:14.648481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# results = {}\n# if training_args.do_eval:\n#   print((\"*** Evaluate ***\"))\n#   metrics = trainer.evaluate(\n#       max_length=64, \n#       num_beams=None, \n#       metric_key_prefix=\"eval\"\n#   )\n#   max_val_samples = len(encoded_val_ds)\n#   metrics[\"eval_samples\"] = min(max_val_samples, len(encoded_val_ds))\n  \n#   if trainer.is_world_process_zero():\n#     metrics_formatted = trainer.metrics_format(metrics)\n#     print(\"***** val metrics *****\")\n#     k_width = max(len(str(x)) for x in metrics_formatted.keys())\n#     v_width = max(len(str(x)) for x in metrics_formatted.values())\n#     for key in sorted(metrics_formatted.keys()):\n#       print(f\"  {key: <{k_width}} = {metrics_formatted[key]:>{v_width}}\")\n#     save_json(metrics, os.path.join(training_args.output_dir, \"eval_results.json\"))\n#     all_metrics.update(metrics)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.649687Z","iopub.status.idle":"2023-11-16T13:14:14.650167Z","shell.execute_reply.started":"2023-11-16T13:14:14.649917Z","shell.execute_reply":"2023-11-16T13:14:14.649938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"用于在验证集上评估训练好的模型，并记录评估指标值。具体流程如下：\n\n首先，判断是否需要进行评估（training_args.do_eval为True）。如果需要，则调用trainer.evaluate()方法，在验证集encoded_val_ds上对当前模型进行评估。\n\nevaluate()方法的参数包括：\n\nmax_length：生成的文本最大长度。\nnum_beams：束搜索（beam search）过程中保留的翻译序列数量。如果为None或小于等于1，则使用贪心搜索（greedy search）。\nmetric_key_prefix：评估指标的前缀，用于区分不同数据集的指标。\n评估完成后，从评估结果metrics中获取评估指标信息，并将最大验证样本数max_val_samples设置为编码后验证数据集的长度。然后，将指标eval_samples设置为实际验证样本数和max_val_samples中较小的值。\n\n如果当前进程为主进程（trainer.is_world_process_zero()为True），则将指标结果进行格式化，并打印出来。然后，将指标保存为JSON文件并更新到all_metrics字典中。最后，将当前评估指标结果metrics保存在results字典中。","metadata":{}},{"cell_type":"code","source":"# if training_args.do_predict:\n#   print((\"*** Test ***\"))\n#   test_results = trainer.predict(\n#       encoded_test_ds,\n#       metric_key_prefix=\"test\",\n#       max_length=64,\n#       num_beams=None,\n#     )\n#   metrics = test_results.metrics\n#   max_test_samples = len(encoded_test_ds)\n#   metrics[\"test_samples\"] = min(max_test_samples, len(encoded_test_ds))\n    \n#   if trainer.is_world_process_zero():\n#     metrics_formatted = trainer.metrics_format(metrics)\n#     print((\"***** test metrics *****\"))\n#     k_width = max(len(str(x)) for x in metrics_formatted.keys())\n#     v_width = max(len(str(x)) for x in metrics_formatted.values())\n#     for key in sorted(metrics_formatted.keys()):\n#       print(f\"  {key: <{k_width}} = {metrics_formatted[key]:>{v_width}}\")\n#     save_json(metrics, os.path.join(training_args.output_dir, \"test_results.json\"))\n#     all_metrics.update(metrics)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.651501Z","iopub.status.idle":"2023-11-16T13:14:14.652007Z","shell.execute_reply.started":"2023-11-16T13:14:14.651732Z","shell.execute_reply":"2023-11-16T13:14:14.651754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"用于在测试集上进行预测，并记录预测结果的评估指标值。具体流程如下：\n\n首先，判断是否需要进行预测（training_args.do_predict为True）。如果需要，则调用trainer.predict()方法，在测试集encoded_test_ds上对当前模型进行预测。\n\npredict()方法的参数包括：\n\nencoded_test_ds：编码后的测试数据集。\nmetric_key_prefix：评估指标的前缀，用于区分不同数据集的指标。\nmax_length：生成的文本最大长度。\nnum_beams：束搜索（beam search）过程中保留的翻译序列数量。如果为None或小于等于1，则使用贪心搜索（greedy search）。\n预测完成后，从预测结果test_results中获取评估指标信息，并将最大测试样本数max_test_samples设置为编码后测试数据集的长度。然后，将指标test_samples设置为实际测试样本数和max_test_samples中较小的值。\n\n如果当前进程为主进程（trainer.is_world_process_zero()为True），则将指标结果进行格式化，并打印出来。然后，将指标保存为JSON文件并更新到all_metrics字典中。\n\n最后，将当前预测结果的评估指标结果metrics保存在results字典中，并进行更新。","metadata":{}},{"cell_type":"code","source":"# from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n\n# # 加载已保存的模型和tokenizer\n# output_model_dir = \"/kaggle/working/\"\n# model = AutoModelForQuestionAnswering.from_pretrained(output_model_dir)\n# tokenizer = AutoTokenizer.from_pretrained(output_model_dir)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.653792Z","iopub.status.idle":"2023-11-16T13:14:14.654255Z","shell.execute_reply.started":"2023-11-16T13:14:14.654025Z","shell.execute_reply":"2023-11-16T13:14:14.654048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"在做下游任务，利用WebQA问答对做数据集，来训练问答系统","metadata":{}},{"cell_type":"code","source":"# # 读取训练集数据\n# with open('/kaggle/input/dataset/WebQA.v1.0/me_train.json', 'r', encoding='utf-8') as file:\n#     training_set = json.load(file)\n\n# # 创建问题和答案列表\n# questions = []\n# answers = []\n\n# # 遍历训练集中的每个问题\n# for question_id, question_data in training_set.items():\n#     question = question_data['question']\n#     evidences = question_data['evidences']\n    \n#     # 遍历每个证据项，将答案和问题添加到列表中\n#     for evidence_id, evidence_data in evidences.items():\n#         answer = evidence_data['answer'][0]\n#         evidence_text = evidence_data['evidence']\n        \n#         questions.append(question)\n#         answers.append(answer)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.655655Z","iopub.status.idle":"2023-11-16T13:14:14.656348Z","shell.execute_reply.started":"2023-11-16T13:14:14.656091Z","shell.execute_reply":"2023-11-16T13:14:14.656117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\n# 读取JSON文件\nwith open('/kaggle/input/webqadataset/WebQA.json', 'r') as file:\n    data = json.load(file)\n\n# 计算要提取的问答对数量\ntotal_items = len(data)\nextract_count = max(total_items // 10000, 1)   # 只提取总数的万分之一（至少提取一个）\n\n# 提取数据\nextracted_data = [{\"question\": item[\"question\"], \"id\": item[\"id\"], \"answer\": item[\"passages\"][0][\"answer\"]} for i, item in enumerate(data) if i % extract_count == 0]\n\n# 将提取的数据保存为新的JSON文件\nwith open('/kaggle/working/WebQA_Extracted.json', 'w') as file:\n    json.dump(extracted_data, file)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.657870Z","iopub.status.idle":"2023-11-16T13:14:14.658363Z","shell.execute_reply.started":"2023-11-16T13:14:14.658110Z","shell.execute_reply":"2023-11-16T13:14:14.658132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\n# 读取JSON文件\nwith open('/kaggle/input/dataset/WebQA.v1.0/me_train.json', 'r') as file:\n    data = json.load(file)\n\n# 计算要提取的数据量\ntotal_data = len(data)\nextracted_data = total_data // 10000\n\n# 提取数据\nkeys = list(data.keys())[:extracted_data]\nextracted_data = {key: data[key] for key in keys}\n\n# 将提取的数据保存为新的JSON文件\nwith open('/kaggle/working/me_train_10000.json', 'w') as file:\n    json.dump(extracted_data, file)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.659791Z","iopub.status.idle":"2023-11-16T13:14:14.660255Z","shell.execute_reply.started":"2023-11-16T13:14:14.660024Z","shell.execute_reply":"2023-11-16T13:14:14.660046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(data))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.662016Z","iopub.status.idle":"2023-11-16T13:14:14.662331Z","shell.execute_reply.started":"2023-11-16T13:14:14.662176Z","shell.execute_reply":"2023-11-16T13:14:14.662190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import json\n# from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n# from torch.utils.data import DataLoader, Dataset\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n\n# # 加载已保存的模型和tokenizer\n# output_model_dir = \"/kaggle/working/\"\n# model = AutoModelForQuestionAnswering.from_pretrained(output_model_dir)\n# tokenizer = AutoTokenizer.from_pretrained(output_model_dir)\n\n# import pandas as pd\n# from torch.utils.data import Dataset\n\n# class DownstreamDataset(Dataset):\n#     def __init__(self, data_file, tokenizer):\n#         try:\n#             with open(data_file, 'r', encoding='utf-8') as f:\n#                 data = json.load(f)\n#                 self.qas_data = pd.DataFrame(data)['qas']  # 获取'qas'列\n#                 self.tokenizer = tokenizer\n#         except Exception as e:\n#             print(\"An error occurred while loading the dataset:\", e)\n\n#     def __len__(self):\n#         return len(self.qas_data)\n\n#     def __getitem__(self, index):\n#         qas_list = self.qas_data[index]  # 获取列表对象\n#         qas_json = json.dumps(qas_list)  # 将列表对象转换为JSON字符串\n#         qas_dict = json.loads(qas_json)  # 解析JSON字符串为Python对象\n#         question = str(qas_dict[0]['query_text'])  # 获取问题文本\n#         answer = str(qas_dict[0].get('answer', ''))  # 获取答案文本，如果不存在则返回空字符串\n#         encoding = self.tokenizer.encode_plus(\n#             question,\n#             answer,\n#             # 其他参数\n#             return_tensors='pt'\n#         )\n#         return {\n#             'input_ids': encoding['input_ids'].flatten(),\n#             'attention_mask': encoding['attention_mask'].flatten(),\n#             # 其他返回值\n#         }\n\n\n# print(1)\n# # 创建下游任务的数据集\n# downstream_dataset = DownstreamDataset('/kaggle/input/dataset2/cmrc2018_train.json', tokenizer)\n# print(1.5)\n# # 创建数据加载器\n# batch_size = 3\n# data_loader = DataLoader(downstream_dataset, batch_size=batch_size, shuffle=True)\n# print(len(data_loader))\n# # 定义损失函数和优化器\n# loss_fn = nn.CrossEntropyLoss()\n# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    \n#     # 将模型设置为训练模式\n# model.train()\n# print(2)\n# # 开始下游任务的训练\n# num_epochs = 10\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model.to(device)\n# print(device)\n# for epoch in range(num_epochs):\n#     running_loss = 0.0\n#     print(len(data_loader))\n#     for batch_idx, batch in enumerate(data_loader):\n#         print(len(data_loader))\n#         batch_input_ids = batch['input_ids'].to(device)\n#         batch_attention_mask = batch['attention_mask'].to(device)\n\n#         optimizer.zero_grad()\n\n#         outputs = model(\n#             input_ids=batch_input_ids,\n#             attention_mask=batch_attention_mask\n#         )\n\n#         loss = outputs.loss\n#         loss.backward()\n#         optimizer.step()\n\n#         running_loss += loss.item()\n\n#         if (batch_idx + 1) % 10 == 0:  # 每处理10个batch打印一次信息\n#             print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item():.4f}\")\n\n#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(data_loader)}\")\n# print(3)\n\n\n# # 训练完成后，你可以保存或使用训练好的模型进行预测或评估","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:20:33.249865Z","iopub.execute_input":"2023-11-16T14:20:33.250593Z","iopub.status.idle":"2023-11-16T14:20:36.042331Z","shell.execute_reply.started":"2023-11-16T14:20:33.250557Z","shell.execute_reply":"2023-11-16T14:20:36.040952Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at /kaggle/working/ and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"1\n1.5\n801\n2\ncuda\n801\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[54], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_loader))\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_loader))\n\u001b[1;32m     73\u001b[0m     batch_input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[54], line 35\u001b[0m, in \u001b[0;36mDownstreamDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     33\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(qas_dict[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_text\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# 获取问题文本\u001b[39;00m\n\u001b[1;32m     34\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(qas_dict[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# 获取答案文本，如果不存在则返回空字符串\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 其他参数\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# 其他返回值\u001b[39;00m\n\u001b[1;32m     45\u001b[0m }\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/deprecated/tapex/tokenization_tapex.py:929\u001b[0m, in \u001b[0;36mTapexTokenizer.encode_plus\u001b[0;34m(self, table, query, answer, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(ENCODE_KWARGS_DOCSTRING, TAPEX_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_plus\u001b[39m(\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    918\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m    920\u001b[0m     padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m    921\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m    922\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    927\u001b[0m     )\n\u001b[0;32m--> 929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43manswer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/deprecated/tapex/tokenization_tapex.py:978\u001b[0m, in \u001b[0;36mTapexTokenizer._encode_plus\u001b[0;34m(self, table, query, answer, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    971\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    972\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    976\u001b[0m     )\n\u001b[0;32m--> 978\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_table_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# if necessary, perform lower case\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/deprecated/tapex/tokenization_tapex.py:1335\u001b[0m, in \u001b[0;36mTapexTokenizer.prepare_table_query\u001b[0;34m(self, table, query, answer, truncation_strategy, max_length)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_table_query\u001b[39m(\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1322\u001b[0m     table,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1327\u001b[0m ):\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;124;03m    This method can be used to linearize a table and add a corresponding query.\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;124;03m    An answer can be provided for more precise truncation.\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m:\n\u001b[1;32m   1336\u001b[0m         \u001b[38;5;66;03m# step 1: create table dictionary\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m         table_content \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(table\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28mlist\u001b[39m(row\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39miterrows()]}\n\u001b[1;32m   1339\u001b[0m         \u001b[38;5;66;03m# step 2: modify table internally\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;66;03m# always truncate table cells based on self.max_cell_length\u001b[39;00m\n\u001b[1;32m   1341\u001b[0m         \u001b[38;5;66;03m# optionally truncate rows if truncation_strategy is set to it\u001b[39;00m\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'empty'"],"ename":"AttributeError","evalue":"'str' object has no attribute 'empty'","output_type":"error"}]},{"cell_type":"code","source":"import json\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer\nfrom torch.utils.data import DataLoader, Dataset\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\n\n# 加载已保存的模型和tokenizer\noutput_model_dir = \"/kaggle/working/\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(output_model_dir)\ntokenizer = AutoTokenizer.from_pretrained(output_model_dir)\n\nclass DownstreamDataset(Dataset):\n    def __init__(self, data_file, tokenizer):\n        try:\n            with open(data_file, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                self.qas_data = pd.DataFrame(data)['qas']  # 获取'qas'列\n                self.tokenizer = tokenizer\n        except Exception as e:\n            print(\"An error occurred while loading the dataset:\", e)\n\n    def __len__(self):\n        return len(self.qas_data)\n\n    def __getitem__(self, index):\n        qas_list = self.qas_data[index]  # 获取列表对象\n        qas_json = json.dumps(qas_list)  # 将列表对象转换为JSON字符串\n        qas_dict = json.loads(qas_json)  # 解析JSON字符串为Python对象\n        question = str(qas_dict[0]['query_text'])  # 获取问题文本\n        answer = str(qas_dict[0].get('answer', ''))  # 获取答案文本，如果不存在则返回空字符串\n        encoding = self.tokenizer.encode_plus(\n            question,\n            answer,\n            add_special_tokens=True,\n            padding='max_length',\n            max_length=512,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n        }\n\n# 创建下游任务的数据集\ndownstream_dataset = DownstreamDataset('/kaggle/input/dataset2/cmrc2018_train.json', tokenizer)\n\n# 创建数据加载器\n# 创建数据加载器\nbatch_size = 3\ndata_loader = DataLoader(downstream_dataset, batch_size=batch_size, shuffle=True)\n\n# 将DataLoader转换为迭代器\ndata_iter = iter(data_loader)\n\n# 开始下游任务的训练\nnum_epochs = 10\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for batch_idx in range(len(data_loader)):  # 使用range(len(data_loader))遍历迭代器\n        batch = next(data_iter)  # 获取下一个数据样本\n\n        batch_input_ids = batch['input_ids'].to(device)\n        batch_attention_mask = batch['attention_mask'].to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(\n            input_ids=batch_input_ids,\n            attention_mask=batch_attention_mask\n        )\n\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        if (batch_idx + 1) % 10 == 0:  # 每处理10个batch打印一次信息\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item():.4f}\")\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(data_loader)}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:36:46.451106Z","iopub.execute_input":"2023-11-16T14:36:46.451500Z","iopub.status.idle":"2023-11-16T14:36:49.277702Z","shell.execute_reply.started":"2023-11-16T14:36:46.451471Z","shell.execute_reply":"2023-11-16T14:36:49.276262Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stderr","text":"Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at /kaggle/working/ and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[60], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_loader)):  \u001b[38;5;66;03m# 使用range(len(data_loader))遍历迭代器\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_iter\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 获取下一个数据样本\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     batch_input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     67\u001b[0m     batch_attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[60], line 33\u001b[0m, in \u001b[0;36mDownstreamDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     31\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(qas_dict[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_text\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# 获取问题文本\u001b[39;00m\n\u001b[1;32m     32\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(qas_dict[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# 获取答案文本，如果不存在则返回空字符串\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m     44\u001b[0m }\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/deprecated/tapex/tokenization_tapex.py:929\u001b[0m, in \u001b[0;36mTapexTokenizer.encode_plus\u001b[0;34m(self, table, query, answer, add_special_tokens, padding, truncation, max_length, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(ENCODE_KWARGS_DOCSTRING, TAPEX_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_plus\u001b[39m(\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    918\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m    920\u001b[0m     padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m    921\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m    922\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    927\u001b[0m     )\n\u001b[0;32m--> 929\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43manswer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/deprecated/tapex/tokenization_tapex.py:978\u001b[0m, in \u001b[0;36mTapexTokenizer._encode_plus\u001b[0;34m(self, table, query, answer, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    971\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    972\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    976\u001b[0m     )\n\u001b[0;32m--> 978\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_table_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# if necessary, perform lower case\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/deprecated/tapex/tokenization_tapex.py:1335\u001b[0m, in \u001b[0;36mTapexTokenizer.prepare_table_query\u001b[0;34m(self, table, query, answer, truncation_strategy, max_length)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_table_query\u001b[39m(\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1322\u001b[0m     table,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1327\u001b[0m ):\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;124;03m    This method can be used to linearize a table and add a corresponding query.\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;124;03m    An answer can be provided for more precise truncation.\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m:\n\u001b[1;32m   1336\u001b[0m         \u001b[38;5;66;03m# step 1: create table dictionary\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m         table_content \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(table\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28mlist\u001b[39m(row\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39miterrows()]}\n\u001b[1;32m   1339\u001b[0m         \u001b[38;5;66;03m# step 2: modify table internally\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;66;03m# always truncate table cells based on self.max_cell_length\u001b[39;00m\n\u001b[1;32m   1341\u001b[0m         \u001b[38;5;66;03m# optionally truncate rows if truncation_strategy is set to it\u001b[39;00m\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'empty'"],"ename":"AttributeError","evalue":"'str' object has no attribute 'empty'","output_type":"error"}]},{"cell_type":"code","source":"import json\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\n\n# 加载已保存的模型和tokenizer\noutput_model_dir = \"/kaggle/working/\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(output_model_dir)\ntokenizer = AutoTokenizer.from_pretrained(output_model_dir)\nos.listdir('/kaggle/input/dataset2/')\n\nclass DownstreamDataset(Dataset):\n    def __init__(self, data_file, tokenizer):\n        try:\n            with open(data_file, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                self.qas_data = pd.DataFrame(data)['qas']  # 获取'qas'列\n                self.tokenizer = tokenizer\n        except Exception as e:\n            print(\"An error occurred while loading the dataset:\", e)\n\n    def __len__(self):\n        return len(self.qas_data)\n\n    def __getitem__(self, index):\n        qas_list = self.qas_data[index]  # 获取列表对象\n        qas_json = json.dumps(qas_list)  # 将列表对象转换为JSON字符串\n        qas_dict = json.loads(qas_json)  # 解析JSON字符串为Python对象\n        question = str(qas_dict[0]['query_text'])  # 获取问题文本\n        answer = str(qas_dict[0].get('answer', ''))  # 获取答案文本，如果不存在则返回空字符串\n        encoding = self.tokenizer.encode_plus(\n            question,\n            answer,\n            add_special_tokens=True,\n            padding='max_length',\n            max_length=512,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n        }\n\n# 创建下游任务的数据集\nimport os\n\nfile_path = '/kaggle/input/dataset2/cmrc2018_train.json'\nif os.path.exists(file_path):\n    print(f\"The file {file_path} exists.\")\nelse:\n    print(f\"The file {file_path} does not exist.\")\n\ndownstream_dataset = DownstreamDataset('/kaggle/input/dataset2/cmrc2018_train.json', tokenizer)\nprint(type(downstream_dataset))\n# 定义训练参数\ntraining_args = TrainingArguments(\n    output_dir=output_model_dir,  # 模型保存路径\n    num_train_epochs=10,  # 训练轮数\n    per_device_train_batch_size=3,  # 每个设备的训练batch size\n    logging_dir='./logs',  # 训练日志保存路径\n    logging_steps=10\n)\n\n# 定义Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=downstream_dataset,\n)\nprint(type(table))\nprint(1)\nif table.empty:\n    print(\"Table is empty\")\nelse:\n    print(\"非空\")\n# 开始训练\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-11-16T15:07:13.308264Z","iopub.execute_input":"2023-11-16T15:07:13.308648Z","iopub.status.idle":"2023-11-16T15:07:16.183551Z","shell.execute_reply.started":"2023-11-16T15:07:13.308622Z","shell.execute_reply":"2023-11-16T15:07:16.182304Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stderr","text":"Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at /kaggle/working/ and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"The file /kaggle/input/dataset2/cmrc2018_train.json exists.\n<class '__main__.DownstreamDataset'>\n<class 'torch.utils.data.dataloader.DataLoader'>\n1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[80], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(table))\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'empty'"],"ename":"AttributeError","evalue":"'DataLoader' object has no attribute 'empty'","output_type":"error"}]},{"cell_type":"code","source":"type(data_loader)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T14:24:51.551880Z","iopub.execute_input":"2023-11-16T14:24:51.552566Z","iopub.status.idle":"2023-11-16T14:24:51.558020Z","shell.execute_reply.started":"2023-11-16T14:24:51.552533Z","shell.execute_reply":"2023-11-16T14:24:51.557116Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"torch.utils.data.dataloader.DataLoader"},"metadata":{}}]},{"cell_type":"code","source":"print(downstream_dataset.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.665521Z","iopub.status.idle":"2023-11-16T13:14:14.665900Z","shell.execute_reply.started":"2023-11-16T13:14:14.665717Z","shell.execute_reply":"2023-11-16T13:14:14.665732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"downstream_dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.667032Z","iopub.status.idle":"2023-11-16T13:14:14.667341Z","shell.execute_reply.started":"2023-11-16T13:14:14.667186Z","shell.execute_reply":"2023-11-16T13:14:14.667200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# 读取整个JSON文件\ndata = pd.read_json('/kaggle/input/dataset/WebQA.v1.0/me_train.json')\n\n# 提取前100条数据\nfirst_100_rows = data.head(1)\n\n# 将前100条数据保存回JSON文件\nfirst_100_rows.to_json('/kaggle/working/me_train_1.json', orient='records')\n\n# 输出提示信息\nprint(\"已将文件保存为：/kaggle/working/me_train_100.json\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.668803Z","iopub.status.idle":"2023-11-16T13:14:14.669258Z","shell.execute_reply.started":"2023-11-16T13:14:14.669028Z","shell.execute_reply":"2023-11-16T13:14:14.669049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# 读取整个JSON文件\ndata = pd.read_json('/kaggle/input/dataset/WebQA.v1.0/me_train.json')\n\n# 提取前100条数据\nfirst_100_rows = data.head(100)\n\n# 打印提取的数据\nprint(first_100_rows)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.670310Z","iopub.status.idle":"2023-11-16T13:14:14.670749Z","shell.execute_reply.started":"2023-11-16T13:14:14.670520Z","shell.execute_reply":"2023-11-16T13:14:14.670541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# 读取数据文件\ndata = pd.read_json('/kaggle/working/me_train_1000.json')\n\n# 打印列名\n# print(data.head())\nprint(data.columns)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.671903Z","iopub.status.idle":"2023-11-16T13:14:14.672338Z","shell.execute_reply.started":"2023-11-16T13:14:14.672105Z","shell.execute_reply":"2023-11-16T13:14:14.672126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 保存训练好的模型\nsave_model_dir = \"/kaggle/working/model\"\nmodel.save_pretrained(save_model_dir)\ntokenizer.save_pretrained(save_model_dir)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.674159Z","iopub.status.idle":"2023-11-16T13:14:14.674521Z","shell.execute_reply.started":"2023-11-16T13:14:14.674351Z","shell.execute_reply":"2023-11-16T13:14:14.674368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"下面这个代码应该还需要进行修改","metadata":{}},{"cell_type":"code","source":"# # 给定的问题\n# question = \"数据收益合理化分配的原则和收益分配机制分别是什么？\"\n\n# answer = model(question, content)[\"answer\"]\n\n# print(answer)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.675772Z","iopub.status.idle":"2023-11-16T13:14:14.676132Z","shell.execute_reply.started":"2023-11-16T13:14:14.675966Z","shell.execute_reply":"2023-11-16T13:14:14.675981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n\n# # 加载已保存的模型和tokenizer\n# output_model_dir = \"/kaggle/working/\"\n# model = AutoModelForQuestionAnswering.from_pretrained(output_model_dir)\n# tokenizer = AutoTokenizer.from_pretrained(output_model_dir)\n\n# # 提取context列\n# context = data['content']\n\n# # 将列表元素连接为单个字符串\n# text = \"\\n\".join(context)\n# import pandas as pd\n\n# # 将文本字符串转换为表格对象\n# text_df = pd.DataFrame([text], columns=['text'])\n\n# question = \"数据收益合理化分配的原则和收益分配机制分别是什么？\"\n\n# # 构建输入数据\n# inputs = tokenizer.encode_plus(text_df, question, padding=True, truncation=True, return_tensors=\"pt\")\n\n# # 在模型上进行推理\n# outputs = model(**inputs)\n\n# # 获取答案\n# answer_start = torch.argmax(outputs.start_logits)\n# answer_end = torch.argmax(outputs.end_logits)\n# answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end+1]))\n\n# # 打印答案\n# print(answer)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.677240Z","iopub.status.idle":"2023-11-16T13:14:14.677603Z","shell.execute_reply.started":"2023-11-16T13:14:14.677400Z","shell.execute_reply":"2023-11-16T13:14:14.677415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.679104Z","iopub.status.idle":"2023-11-16T13:14:14.679416Z","shell.execute_reply.started":"2023-11-16T13:14:14.679260Z","shell.execute_reply":"2023-11-16T13:14:14.679275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # import torch\n# # from transformers import BartTokenizer\n\n# # # 加载模型和分词器\n# # model = BartForQuestionAnswering.from_pretrained(\"model_name\")\n# # tokenizer = BartTokenizer.from_pretrained(\"model_name\")\n\n# # 加载数据集\n# #data = pd.read_csv(\"your_dataset.csv\")  # 将 \"your_dataset.csv\" 替换为你的数据集文件名\n# # 将表格数据合并为一个单独的 DataFrame 对象\n# table_example = pd.concat(data, axis=0, ignore_index=True)\n# # 定义问题\n# question = \"数据收益合理化分配的原则和收益分配机制分别是什么？\"\n\n# # 将表格示例转换为文本格式\n# table_text = table_example.to_string(index=False)\n\n# # 使用 tokenizer 进行问答\n# inputs = tokenizer(question, table_text, truncation=True, padding=True, return_tensors=\"pt\")\n# outputs = model(**inputs)\n\n# # 解码答案\n# answer = tokenizer.decode(outputs.start_logits, inputs[\"attention_mask\"])\n\n# # 打印答案\n# print(answer)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.680603Z","iopub.status.idle":"2023-11-16T13:14:14.681097Z","shell.execute_reply.started":"2023-11-16T13:14:14.680857Z","shell.execute_reply":"2023-11-16T13:14:14.680880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import BartTokenizer, BartForQuestionAnswering\n\n# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n# model = BartForQuestionAnswering.from_pretrained('facebook/bart-large')\n\nquestion = \"数据收益合理化分配的原则和收益分配机制分别是什么？\"\ninputs = tokenizer.encode_plus(question, return_tensors='pt')\n\ninput_ids = inputs['input_ids']\nattention_mask = inputs['attention_mask']\n\noutputs = model(input_ids, attention_mask=attention_mask)\n\nstart_logits = outputs.start_logits\nend_logits = outputs.end_logits\n\nstart_index = torch.argmax(start_logits)\nend_index = torch.argmax(end_logits)\n\nanswer_tokens = input_ids[0][start_index:end_index+1]\nanswer = tokenizer.decode(answer_tokens)\nprint(answer)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.683022Z","iopub.status.idle":"2023-11-16T13:14:14.683516Z","shell.execute_reply.started":"2023-11-16T13:14:14.683239Z","shell.execute_reply":"2023-11-16T13:14:14.683273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom bert_score import score\n\n# 假设answers是一个包含所有回答的列表，reference是正确答案\nanswers = [\"第一个回答\", \"第二个回答\", \"第三个回答\", ...]  # 替换为实际的回答列表\nreference = \"正确的答案\"  # 替换为实际的正确答案\n\nall_scores = []\nfor ans in answers:\n    P, R, F1 = score([ans], [reference], lang=\"zh\", rescale_with_baseline=True)\n    all_scores.append(F1.item())\n\naverage_score = sum(all_scores) / len(all_scores)\nprint(\"每个回答的BERTScore：\", all_scores)\nprint(\"平均BERTScore：\", average_score)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-16T13:14:14.685088Z","iopub.status.idle":"2023-11-16T13:14:14.685590Z","shell.execute_reply.started":"2023-11-16T13:14:14.685321Z","shell.execute_reply":"2023-11-16T13:14:14.685344Z"},"trusted":true},"execution_count":null,"outputs":[]}]}